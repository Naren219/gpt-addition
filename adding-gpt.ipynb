{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**task:** train a GPT to do addition of two numbers, i.e. a+b=c. You may find it helpful to predict the digits of c in reverse order, as the typical addition algorithm (that you're hoping it learns) would proceed right to left too. You may want to modify the data loader to simply serve random problems and skip the generation of train.bin, val.bin. You may want to mask out the loss at the input positions of a+b that just specify the problem using y=-1 in the targets (see CrossEntropyLoss ignore_index)."
      ],
      "metadata": {
        "id": "2LWVividAom8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## two approaches: seq-to-seq and autoregressive prediction\n",
        "1. predicting the entire output (sum) from the inputs (13+35 for ex) --> memorizing the mapping from an input format to an output\n",
        "2. predicting the next number after shifting the window to the right"
      ],
      "metadata": {
        "id": "KsWcsFgc-6cJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "XhYkvTQ2_84z"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nums = [str(i) for i in range(10)] + list(\"+=;\")\n",
        "print(len(nums))\n",
        "''.join(nums)"
      ],
      "metadata": {
        "id": "PiI3vIvNI7oS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "f2f687e2-6a91-4524-d9cd-8fb14dc71bb7"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "13\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'0123456789+=;'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = len(nums)\n",
        "torch.manual_seed(1337)\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(device)\n",
        "eval_iters = 200\n",
        "eval_interval = 100\n",
        "train_ratio = 0.8\n",
        "max_digits = 2\n",
        "\n",
        "block_size = 120\n",
        "batch_size = 8\n",
        "learning_rate = 1e-3\n",
        "max_iters = 20000\n",
        "dataset_size = 10000\n",
        "\n",
        "n_embd = 32\n",
        "dropout = 0.2\n",
        "n_head = 4\n",
        "n_layer = 4"
      ],
      "metadata": {
        "id": "NccBnhcuASV7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f9bd6869-622a-41e0-d4ea-e80bcd15ce2e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stoi = { ch:i for i,ch in enumerate(nums) }\n",
        "itos = { i:ch for i,ch in enumerate(nums) }\n",
        "encode = lambda s: [stoi[c] for c in s]\n",
        "decode = lambda l: ''.join([itos[i] for i in l])\n",
        "\n",
        "print(encode(\"123;+0\"))\n",
        "print(decode(encode(\"123;+0\")))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UlNsJXMpc4aU",
        "outputId": "369a6dbb-f619-4612-ac34-ab6c9be484aa"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 2, 3, 12, 10, 0]\n",
            "123;+0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "samples = \"\"\n",
        "\n",
        "for i in range(dataset_size):\n",
        "    a = random.randint(10**(max_digits-1), 10**max_digits - 1)\n",
        "    b = random.randint(10**(max_digits-1), 10**max_digits - 1)\n",
        "\n",
        "    a_padded = str(a).zfill(max_digits)\n",
        "    b_padded = str(b).zfill(max_digits)\n",
        "\n",
        "    sol = str(a + b).zfill(max_digits + 1)[::-1] # pad extra values and reverse solution\n",
        "    new = a_padded + \"+\" + b_padded + \"=\" + sol + \";\"\n",
        "    samples += new\n",
        "\n",
        "print(samples[:50])\n",
        "equations = encode(samples)"
      ],
      "metadata": {
        "id": "t6mG4AwwlhHc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0b7812e9-8dd9-4091-e569-22f931d4fe3f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "72+84=651;78+37=511;65+35=001;69+36=501;13+78=190;\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "only want to look at target values where the respective x sample has '=' and a complete input block (eg. 12+24) (which has length = `max_digits*2 + 1`). else, we'd be training with equation having incomplete terms."
      ],
      "metadata": {
        "id": "SHCy2FM5mWUi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "split_index = int(train_ratio * len(equations))\n",
        "train_set = equations[:split_index]\n",
        "test_val = equations[split_index:]\n",
        "\n",
        "half = len(test_val) // 2\n",
        "test_set = test_val[:half]\n",
        "val_set = test_val[half:]"
      ],
      "metadata": {
        "id": "nasDna_RELKJ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_batch(data):\n",
        "  data = torch.tensor(data)\n",
        "  ix = torch.randint(0, len(data) - block_size, (batch_size,))\n",
        "  x = torch.stack([data[i : i + block_size] for i in ix])\n",
        "  y = torch.stack([data[i + 1 : i + block_size + 1] for i in ix])\n",
        "\n",
        "  # Mask y based on the position of ';' and '=' in x\n",
        "  for i in range(batch_size):\n",
        "    eq_indices = [i for i, val in enumerate(x[i]) if val == encode('=')[0]]\n",
        "    semicolon_indices = [i for i, val in enumerate(x[i]) if val == encode(';')[0]]\n",
        "    # Initialize mask with -1\n",
        "    mask = torch.ones_like(y[i]) * -1\n",
        "\n",
        "    # Unmask y for segments between ';' and '='\n",
        "    for j in range(len(eq_indices)):\n",
        "        if j < len(semicolon_indices):\n",
        "            if eq_indices[j] < semicolon_indices[j]:\n",
        "                start = eq_indices[j] - 1\n",
        "                end = semicolon_indices[j] + 1\n",
        "            else:\n",
        "                start = semicolon_indices[j] - 1\n",
        "                end = eq_indices[j] + 1\n",
        "            mask[start:end] = y[i, start:end]  # Unmask the segment between ';' and '='\n",
        "\n",
        "    # Unmask everything after the last equal sign\n",
        "    if eq_indices[-1] > semicolon_indices[-1]:\n",
        "        mask[eq_indices[-1] - 1 :] = y[i, eq_indices[-1] - 1 :]\n",
        "\n",
        "    y[i] = mask\n",
        "\n",
        "  x, y = x.to(device), y.to(device)\n",
        "  return x, y"
      ],
      "metadata": {
        "id": "IVCGF5gIUWvp"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_batch2(data):\n",
        "    data = torch.tensor(data)\n",
        "    ix = torch.randint(0, len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i : i + block_size] for i in ix])\n",
        "    y = torch.stack([data[i + 1 : i + block_size + 1] for i in ix])\n",
        "\n",
        "    eq_token = encode('=')[0]\n",
        "    semicolon_token = encode(';')[0]\n",
        "\n",
        "    # Mask y based on the position of ';' and '=' in x\n",
        "    for i in range(batch_size):\n",
        "        # Initialize mask with -1\n",
        "        mask = torch.ones_like(y[i]) * -1\n",
        "\n",
        "        equation_start = 0\n",
        "        for j in range(block_size):\n",
        "            token = x[i, j].item()\n",
        "\n",
        "            # If semicolon or end of sequence is encountered, process the equation\n",
        "            if token == semicolon_token or j == block_size - 1:\n",
        "                equation = x[i, equation_start : j + 1].tolist()\n",
        "                equation_start = j + 1  # Update start for the next equation\n",
        "\n",
        "                # Find the '=' position in the equation\n",
        "                if eq_token in equation:\n",
        "                    eq_idx = equation.index(eq_token)\n",
        "                    solution_start = eq_idx + 1\n",
        "\n",
        "                    # Unmask all digits after '=' in y\n",
        "                    print(decode(equation))\n",
        "                    for k in range(solution_start, len(equation)):\n",
        "                      if encode('0')[0] <= equation[k] <= encode('9')[0]:  # Check if token is a digit\n",
        "                          print(equation_start, equation[k])\n",
        "                          mask[equation_start - len(equation) + k] = y[i, equation_start - len(equation) + k]\n",
        "\n",
        "        # Apply the updated mask to y\n",
        "        y[i] = mask\n",
        "\n",
        "    x, y = x.to(device), y.to(device)\n",
        "\n",
        "    return x, y"
      ],
      "metadata": {
        "id": "upG7d9milAF3"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = encode(\"2+61=321;27+69=690;2\")\n",
        "block_size = 10\n",
        "batch_size = 1\n",
        "\n",
        "# Generate batch\n",
        "a, b = get_batch2(data)\n",
        "\n",
        "print(decode(a[0][:20].tolist()))  # Decoded input sequence\n",
        "print(b[0][:20])                  # Masked target sequence"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zzvZ-MqPlzMx",
        "outputId": "ca8a20e3-401f-4565-d3ce-026eb99a3bd6"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2+61=321;2\n",
            "tensor([-1, -1,  1, 11, -1,  2,  1, 12, -1, -1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "  out = {}\n",
        "  m.eval()\n",
        "\n",
        "  losses = torch.zeros(eval_iters)\n",
        "  for k in range(eval_iters):\n",
        "      X, y = get_batch(train_set)\n",
        "      logits, loss = m(X, y)\n",
        "      losses[k] = loss.item()\n",
        "  out['train'] = losses.mean()\n",
        "\n",
        "  losses = torch.zeros(eval_iters)\n",
        "  for k in range(eval_iters):\n",
        "      X, y = get_batch(val_set)\n",
        "      logits, loss = m(X, y)\n",
        "      losses[k] = loss.item()\n",
        "  out['val'] = losses.mean()\n",
        "\n",
        "  m.train()\n",
        "\n",
        "  return out"
      ],
      "metadata": {
        "id": "x1KoWF7JILKA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CausalSelfAttention(nn.Module):\n",
        "  \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
        "\n",
        "  def __init__(self, num_heads, head_size):\n",
        "      super().__init__()\n",
        "      self.proj = nn.Linear(n_embd, n_embd)\n",
        "      self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "      self.key = nn.Linear(n_embd, head_size*n_head, bias=False)\n",
        "      self.query = nn.Linear(n_embd, head_size*n_head, bias=False)\n",
        "      self.value = nn.Linear(n_embd, head_size*n_head, bias=False)\n",
        "      self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "      self.head_size = head_size\n",
        "\n",
        "  def forward(self, x):\n",
        "      B,T,C = x.shape\n",
        "\n",
        "      k = self.key(x).view(B, n_head, T, C // n_head)\n",
        "      q = self.query(x).view(B, n_head, T, C // n_head)\n",
        "\n",
        "      wei = q @ k.transpose(-2, -1) * C**-0.5\n",
        "      wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')).unsqueeze(0).unsqueeze(0) # wei.shape (1, 1, T, T)\n",
        "      wei = F.softmax(wei, dim=-1)\n",
        "      wei = self.dropout(wei)\n",
        "\n",
        "      v = self.value(x).view(B, n_head, T, C // n_head)\n",
        "      out = wei @ v # (1, 1, T, T) @ (B, nH, T, hS) ---> (B, nH, T, hS)\n",
        "      out = out.transpose(1, 2).contiguous().view(B, T, C)\n",
        "      out = self.dropout(self.proj(out))\n",
        "\n",
        "      return out\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "  def __init__(self, n_embd):\n",
        "    super().__init__()\n",
        "    self.net = nn.Sequential(\n",
        "        nn.Linear(n_embd, 4 * n_embd),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(4 * n_embd, n_embd),\n",
        "        nn.Dropout(dropout)\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.net(x)\n",
        "\n",
        "class Block(nn.Module):\n",
        "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_head\n",
        "        self.sa = CausalSelfAttention(n_head, head_size)\n",
        "        self.ffwd = FeedForward(n_embd)\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "        return x"
      ],
      "metadata": {
        "id": "7vMb7xsSTUVV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.token_embedding_table = nn.Embedding(vocab_size, n_embd, device=device)\n",
        "    self.position_embedding_table = nn.Embedding(block_size, n_embd, device=device)\n",
        "    self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
        "    self.ln_f = nn.LayerNorm(n_embd)\n",
        "    self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "  def forward(self, idx, targets=None):\n",
        "    B, T = idx.shape\n",
        "    tok_emb = self.token_embedding_table(idx) # (B, T, C)\n",
        "    pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T, C)\n",
        "    x = tok_emb + pos_emb\n",
        "    x = self.blocks(x) # (B,T,C)\n",
        "    x = self.ln_f(x) # (B,T,C)\n",
        "    logits = self.lm_head(x) # (B,T,vocab_size)\n",
        "\n",
        "    if targets is None:\n",
        "      loss = None\n",
        "    else:\n",
        "      B, T, C = logits.shape\n",
        "      logits = logits.view(B*T, C)\n",
        "      targets = targets.view(B*T)\n",
        "      loss = F.cross_entropy(logits, targets, ignore_index=-1)\n",
        "    return logits, loss\n",
        "\n",
        "  def generate(self, idx, max_new_tokens):\n",
        "    for _ in range(max_new_tokens):\n",
        "        idx_cond = idx[:, -block_size:]\n",
        "        logits, loss = self(idx_cond)\n",
        "        logits = logits[:, -1, :]\n",
        "        probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "        idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "        idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "    return idx\n",
        "\n",
        "model = BigramLanguageModel()\n",
        "m = model.to(device)"
      ],
      "metadata": {
        "id": "tKZZOaz_Ta5B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "try:\n",
        "  m.load_state_dict(torch.load(\"model.pt\", map_location=torch.device(device)))\n",
        "  print(\"Model loaded successfully!\")\n",
        "\n",
        "except FileNotFoundError:\n",
        "  print(\"Model file not found. Starting with a new model.\")\n",
        "  optimizer = torch.optim.AdamW(m.parameters(), lr=learning_rate)\n",
        "\n",
        "  losses = []\n",
        "\n",
        "  for iter in range(max_iters):\n",
        "    # every once in a while evaluate the loss on train and val sets w/o backprop (hence no_grad)\n",
        "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "    xb, yb = get_batch(train_set)\n",
        "    logits, loss = m(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "  torch.save(m.state_dict(), \"model.pt\")"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "yk2hcie3EWkr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "idx_start = encode(\"57+24=\") # 57 + 24 = 81 = reverse(180)\n",
        "idx = torch.tensor([idx_start], dtype=torch.long, device=device)\n",
        "\n",
        "generated = m.generate(idx, max_new_tokens=20)\n",
        "print(\"Generated Output:\", decode(generated[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YuSHsoq57MGn",
        "outputId": "089bf395-63cd-4203-d62a-d85a59fefc61"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Output: 0+1;62=390;24+90;77=4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "working gpt adder (not mine) - https://colab.research.google.com/drive/1AQ0a8lomUsMkZ2QoUwIPyYKz1PMyu1VS"
      ],
      "metadata": {
        "id": "SOZ0DhkS5etM"
      }
    }
  ]
}